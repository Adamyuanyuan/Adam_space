# 2016-6-3 Spark源码阅读笔记：SparkContext的初始化1——SparkConf以及执行环境SparkEnv的初始化
看书读源码个人笔记，以代码阅读过程中的简记为主，点到为止，加快速度，意会即可
Spark 版本：1.6.1
-----------------

### SparkConf
相当于用户操作Spark的面板，Spark调优的参数就是通过这个类为入口设置

1. 通过ConcurrentHashMap来维护各种Spark的配置属性，以`spark.`开头；
2. 默认从系统获取这些属性，通过一个·loadDefaults·来确定；
3. 有很多 `set` `get`的方法，`configsWithAlternatives`函数用来告诉用户，当前版本的该参数的变化情况

### SparkContext

这可能是Spark中最长最难啃的类了，但是作为基础一定要认真看懂；
最开始是SparkContext的各种构造函数，通过重载

#### 1. 对SparkConf进行初始化，以及一些列参数校验
1.6.1版本的做法如下：（与旧版本的Spark的执行方式不同）

	// 首先定义各种私有变量
	private var _conf: SparkConf = _
	...
	// 定义getConf方法
	def getConf: SparkConf = conf.clone()
	...
	// 代码默认执行程序中，会对定义的私有变量进行初始化,以及一些配置信息的校验
	_conf = config.clone()
    _conf.validateSettings()

    if (!_conf.contains("spark.master")) {
      throw new SparkException("A master URL must be set in your configuration")
    }
    if (!_conf.contains("spark.app.name")) {
      throw new SparkException("An application name must be set in your configuration")
    }

    // System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster
    // yarn-standalone is deprecated, but still supported
    if ((master == "yarn-cluster" || master == "yarn-standalone") &&
        !_conf.contains("spark.yarn.app.id")) {
      throw new SparkException("Detected yarn-cluster mode, but isn't running on a cluster. " +
        "Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.")
    }

    if (_conf.getBoolean("spark.logConf", false)) {
      logInfo("Spark configuration:\n" + _conf.toDebugString)
    }

    // Set Spark driver host and port system properties
    _conf.setIfMissing("spark.driver.host", Utils.localHostName())
    _conf.setIfMissing("spark.driver.port", "0")

    _conf.set("spark.executor.id", SparkContext.DRIVER_IDENTIFIER)
	...
	// 然后在SparkContext类最后的最后，定义Backend的时候也会调用getConf方法
	val backend = new LocalBackend(sc.getConf, scheduler, 1)
	...

#### 设置日志目录与压缩格式_eventLogDir

	_eventLogDir =
      if (isEventLogEnabled) {
        val unresolvedDir = conf.get("spark.eventLog.dir", EventLoggingListener.DEFAULT_LOG_DIR)
          .stripSuffix("/")
        Some(Utils.resolveURI(unresolvedDir))
      } else {
        None
      }

    _eventLogCodec = {
      val compress = _conf.getBoolean("spark.eventLog.compress", false)
      if (compress && isEventLogEnabled) {
        Some(CompressionCodec.getCodecName(_conf)).map(CompressionCodec.getShortName)
      } else {
        None
      }
    }

#### 创建监听器模型listenerBus
_jobProgressListener监听器应该在SparkEnv创建之前来创建

    _jobProgressListener = new JobProgressListener(_conf)
    listenerBus.addListener(jobProgressListener)


#### 创建执行环境SparkEnv
初始化方式和上述同理，就是先定义私有变量 `_env`，然后定义`createSparkEnv`方法，最后调用该方法，算了，还是先把代码贴上去吧，以后就不贴这种代码了，只贴核心代码：

    // 定义私有变量 `_`指该类型变量的初始值
    private var _env: SparkEnv = _
    ...
    private[spark] def env: SparkEnv = _env

    // This function allows components created by SparkEnv to be mocked in unit tests:
    // 核心就是调用 `SparkEnv.createDriverEnv`，有四个参数，其中`listenerBus`参数采用监听器模式维护各类事件的处理，第四个参数就是local模式下driver计算用的核数，其它模式下为0，driver is not used for execution
	private[spark] def createSparkEnv(
		conf: SparkConf,
		isLocal: Boolean,
		listenerBus: LiveListenerBus): SparkEnv = {
		// 仅是Driver端创建SparkEnv
		SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master))
	}

	// Create the Spark execution environment (cache, map output tracker, etc)
	// 类的主程序中调用前面定义的createSparkEnv方法
    _env = createSparkEnv(_conf, isLocal, listenerBus)
    SparkEnv.set(_env)


##### SparkEnv——Spark执行环境详解
这个类是Spark的执行环境对象，有一些参数，上面调用的`SparkEnv.createDriverEnv`方法最终会调用create创建SparkEnv，create方法供driver或者executor来调用；
SparkEnv的构造步骤如下：

    // 1. 创建安全管理器securityManager
	val securityManager = new SecurityManager(conf)

    // Create the ActorSystem for Akka and get the port it binds to.
    // 2. 创建基于Akka的分布式消息系统ActorSystem
    val actorSystemName = if (isDriver) driverActorSystemName else executorActorSystemName
    ... 
    // 然后由actorSystem和SecurityManager封装成AkkaRpcEnv，它除了AKKA还支持netty
    private[spark] class AkkaRpcEnv private[akka] (
	    val actorSystem: ActorSystem,
	    val securityManager: SecurityManager,
	    conf: SparkConf,
	    boundPort: Int)
    	extends RpcEnv(conf) with Logging {
    	...}

    // 然后使用AkkaUtil工具类创建ActorSystem
    ...

    // 3. 创建序列化处理器serializer和结束序列化处理器closureSerializer
     val serializer = instantiateClassFromConf[Serializer](
      "spark.serializer", "org.apache.spark.serializer.JavaSerializer")
    logDebug(s"Using serializer: ${serializer.getClass}")

    val closureSerializer = instantiateClassFromConf[Serializer](
      "spark.closure.serializer", "org.apache.spark.serializer.JavaSerializer")
    ...
    // 4. 创建map任务输出跟踪器mapOutputTracker，里面主要有个k-v结构存储 shuffleId --> Array[MapStatus],以便于reduce阶段获取中间结果输出的地址；

	def registerOrLookupEndpoint(
        name: String, endpointCreator: => RpcEndpoint):
      RpcEndpointRef = {
      if (isDriver) {
        logInfo("Registering " + name)
        rpcEnv.setupEndpoint(name, endpointCreator)
      } else {
        RpcUtils.makeDriverRef(name, conf, rpcEnv)
      }
    }

    val mapOutputTracker = if (isDriver) {
      new MapOutputTrackerMaster(conf)
    } else {
      new MapOutputTrackerWorker(conf)
    }

    // Have to assign trackerActor after initialization as MapOutputTrackerActor
    // requires the MapOutputTracker itself
    mapOutputTracker.trackerEndpoint = registerOrLookupEndpoint(MapOutputTracker.ENDPOINT_NAME,
      new MapOutputTrackerMasterEndpoint(
        rpcEnv, mapOutputTracker.asInstanceOf[MapOutputTrackerMaster], conf))

    // 5. 实例化ShuffleManager，通过反射机制，默认是sort，可以通过参数改为hash
    // shuffleManager用于将map结果写入本地，并写入索引文件，以及从远程或者本地节点读取文件
    // 6. 启动内存管理器memoryManager，最新版本统一为 UnifiedMemoryManager，可以相互借用内存
    // 7. 块传输服务blockManagerMaster，1.6.1版本默认使用Netty
    // 8. 启动 blockManagerMaster，负责对Block的管理和协调，也是创建Actor并注册到ActorSystem中
    // 9. 启动块管理器blockManager，负责对Block的管理
    // 10. 创建广播管理器broadcastManager
    // 11. 创建动态缓存管理器cacheManager
    // 12. 创建测量系统metricsSystem
    // 13. 创建临时目录sparkFilesDir，driver端与executor端的路径不一样
    // 14. 创建输出权利协调器outputCommitCoordinator，最新版本新功能，用来决定每个task是否可以将数据输入到hdfs

    // 15. 在所有的基础组件准备好之后，就创建SparkEnv
    val envInstance = new SparkEnv(
      executorId,
      rpcEnv,
      actorSystem,
      serializer,
      closureSerializer,
      cacheManager,
      mapOutputTracker,
      shuffleManager,
      broadcastManager,
      blockTransferService,
      blockManager,
      securityManager,
      sparkFilesDir,
      metricsSystem,
      memoryManager,
      outputCommitCoordinator,
      conf)

     // driver端最后要删除之前创建的临时目录
     if (isDriver) {
      envInstance.driverTmpDirToDelete = Some(sparkFilesDir)
    }


##### 安全管理器SecurityManager
希望下一个版本能够支持Kerberos
`We currently use DIGEST-MD5 but this could be changed to use Kerberos or other in the future`

#### 创建SPARK_CONTEXT类型的_metadataCleaner
功能是清除过期的持久化RDD

